# LangChain ê¸°ë³¸ê¸° í•™ìŠµ ë…¸íŠ¸

## ğŸ“š ê°•ì˜ ì •ë³´
- **ê°•ì˜ëª…**: í•œì‹œê°„ìœ¼ë¡œ ëë‚´ëŠ” LangChain ê¸°ë³¸ê¸°

## ğŸ“– í•™ìŠµ ëª©ì°¨
### LLMì„ í™œìš©í•´ì„œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ë²•

ollama í™ˆí˜ì´ì§€ì—ì„œ app ë‹¤ìš´ë¡œë“œ ë˜ëŠ” brewë¡œ ì„¤ì¹˜
```bash
brew install ollama
ollama pull gemma3:1b
ollama serve
```

ollama ì‚¬ìš©ë²• ì°¸ê³ 
```bash
Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
```

#### python ê°€ìƒ í™˜ê²½ ì„¤ì •
ê°•ì˜ì—ì„  pyenvë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜ uvë¡œ ëŒ€ì²´í•¨
```bash
uv venv --python 3.11.9 .venv
source .venv/bin/activate
uv pip install --upgrade pip setuptools wheel

python -V                     # Python 3.11.9
python -m pip --version
```

#### 1. Ollamaë¥¼ í™œìš©í•œ ë¡œì»¬ LLM ì‚¬ìš©

```python
# langchain-ollama íŒ¨í‚¤ì§€ ì„¤ì¹˜
%pip install -q langchain-ollama

# ChatOllamaì„ ì‚¬ìš©í•œ ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ
from langchain_ollama import ChatOllama

llm = ChatOllama(model="gemma3:1b")
response = llm.invoke("ë„ˆëŠ” ëˆ„êµ¬ëƒ?")

# ì‘ë‹µ ì˜ˆì‹œ:
# AIMessage(content='ì €ëŠ” Google DeepMindì—ì„œ í›ˆë ¨í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.', 
#           additional_kwargs={}, 
#           response_metadata={'model': 'gemma3:1b', 'created_at': '2025-08-30T22:03:57.432788Z', 'done': True, 'done_reason': 'stop', 'total_duration': 237999125, 'load_duration': 91485041, 'prompt_eval_count': 14, 'prompt_eval_duration': 31124917, 'eval_count': 16, 'eval_duration': 114957250, 'model_name': 'gemma3:1b'}, 
#           id='run--669fd310-5541-4378-838a-60c3e57033e6-0', 
#           usage_metadata={'input_tokens': 14, 'output_tokens': 16, 'total_tokens': 30})
```

#### 2. OpenAI APIë¥¼ í™œìš©í•œ í´ë¼ìš°ë“œ LLM ì‚¬ìš©

```python
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
%pip install -q langchain-openai python-dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ChatOpenAIì„ ì‚¬ìš©í•œ OpenAI ëª¨ë¸ í˜¸ì¶œ
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")  # OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”
response = llm.invoke("ë„ˆëŠ” ëˆ„êµ¬ëƒ?")

# ì‘ë‹µ ì˜ˆì‹œ:
# AIMessage(content='ì €ëŠ” OpenAIì—ì„œ ê°œë°œí•œ ì¸ê³µì§€ëŠ¥ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•˜ê±°ë‚˜ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë° ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', 
#           additional_kwargs={'refusal': None}, 
#           response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 12, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CANy3k9BvSD6ik1TSsYgfSnCx78oB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, 
#           id='run--9e87536c-a51a-490b-8fd9-36efe9c51f3f-0', 
#           usage_metadata={'input_tokens': 12, 'output_tokens': 38, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

#### 3. Azure OpenAI ì‚¬ìš© (ì£¼ì˜ì‚¬í•­)

```python
# Azure OpenAI ì‚¬ìš© ì‹œ ì˜¬ë°”ë¥¸ ì„¤ì • í•„ìš”
from langchain_openai import AzureOpenAI

# ì˜¬ë°”ë¥¸ ì„¤ì • ì˜ˆì‹œ:
llm = AzureOpenAI(
    azure_deployment="your-deployment-name",  # Azure ë°°í¬ ì´ë¦„
    openai_api_version="2024-02-15-preview",  # API ë²„ì „
    azure_endpoint="https://your-resource.openai.azure.com/"  # Azure ì—”ë“œí¬ì¸íŠ¸
)

# ì˜ëª»ëœ ì„¤ì •ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜:
# APIConnectionError: Connection error.
# UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.
```

#### 4. ì£¼ìš” ì°¨ì´ì  ë° íŠ¹ì§•

| êµ¬ë¶„ | Ollama (ë¡œì»¬) | OpenAI (í´ë¼ìš°ë“œ) | Azure OpenAI |
|------|----------------|-------------------|--------------|
| **ì„¤ì¹˜** | ë¡œì»¬ ì„¤ì¹˜ í•„ìš” | API í‚¤ë§Œ í•„ìš” | Azure ë¦¬ì†ŒìŠ¤ í•„ìš” |
| **ë¹„ìš©** | ë¬´ë£Œ | í† í°ë‹¹ ê³¼ê¸ˆ | Azure ì‚¬ìš©ëŸ‰ ê³¼ê¸ˆ |
| **ì„±ëŠ¥** | í•˜ë“œì›¨ì–´ ì˜ì¡´ | ê³ ì„±ëŠ¥ | ê³ ì„±ëŠ¥ |
| **í”„ë¼ì´ë²„ì‹œ** | ì™„ì „ ë¡œì»¬ | OpenAI ì„œë²„ | Azure ì„œë²„ |
| **ëª¨ë¸** | ì œí•œì  | ë‹¤ì–‘í•œ ëª¨ë¸ | OpenAI ëª¨ë¸ |

#### 5. í™˜ê²½ ì„¤ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Python 3.11.9 ì´ìƒ ì„¤ì¹˜ (uv ì‚¬ìš© ê¶Œì¥)
- [ ] ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™” (`uv venv --python 3.11.9 .venv`)
- [ ] í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (langchain-ollama, langchain-openai, python-dotenv)
- [ ] í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼)
- [ ] Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰ (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ)
- [ ] API í‚¤ ì„¤ì • (í´ë¼ìš°ë“œ ëª¨ë¸ ì‚¬ìš© ì‹œ)

### ë­ì²´ì¸ ìŠ¤íƒ€ì¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‘ì„±í•˜ëŠ” ë°©ë²•

#### 1. ê¸°ë³¸ ê°œë… ë° êµ¬ì¡°

LangChainì€ LLMê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ì²´ê³„í™”í•˜ê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ë¡œ ë§Œë“œëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

**í•µì‹¬ êµ¬ì„± ìš”ì†Œ:**
- **LLM (Large Language Model)**: ì‹¤ì œ AI ëª¨ë¸ (Ollama, OpenAI, Azure ë“±)
- **Prompt**: LLMì—ê²Œ ì „ë‹¬í•˜ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸
- **Chain**: ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±
- **Output Parser**: LLMì˜ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜

#### 2. ê¸°ë³¸ LLM í˜¸ì¶œ íŒ¨í„´

```python
# 1. ëª¨ë¸ ì´ˆê¸°í™”
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI

# ë¡œì»¬ ëª¨ë¸ (Ollama)
llm_local = ChatOllama(model="gemma3:1b")

# í´ë¼ìš°ë“œ ëª¨ë¸ (OpenAI)
llm_cloud = ChatOpenAI(model="gpt-4o-mini")

# 2. ê¸°ë³¸ í˜¸ì¶œ
response = llm.invoke("ë„ˆëŠ” ëˆ„êµ¬ëƒ?")
print(response.content)  # ì‘ë‹µ ë‚´ìš©ë§Œ ì¶”ì¶œ
```

#### 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™œìš©

```python
from langchain_core.prompts import ChatPromptTemplate

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
prompt = ChatPromptTemplate.from_template(
    "ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤. {task}ì— ëŒ€í•´ {style}ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
)

# í…œí”Œë¦¿ì— ê°’ ì±„ìš°ê¸°
formatted_prompt = prompt.format_messages(
    role="ì „ë¬¸ í”„ë¡œê·¸ë˜ë¨¸",
    task="Python ê°€ìƒí™˜ê²½ ì„¤ì •",
    style="ë‹¨ê³„ë³„ë¡œ ìì„¸í•˜ê²Œ"
)

# LLMì— ì „ë‹¬
response = llm.invoke(formatted_prompt)
```

#### 4. ì²´ì¸(Chain) êµ¬ì„±

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ê°„ë‹¨í•œ ì²´ì¸ êµ¬ì„±
chain = (
    {"question": RunnablePassthrough()} 
    | prompt 
    | llm 
    | StrOutputParser()
)

# ì²´ì¸ ì‹¤í–‰
result = chain.invoke("Python ê°€ìƒí™˜ê²½ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”")
```

#### 5. ì¶œë ¥ íŒŒì‹± ë° êµ¬ì¡°í™”

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List

# ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì •ì˜
class CodeExample(BaseModel):
    language: str = Field(description="í”„ë¡œê·¸ë˜ë° ì–¸ì–´")
    code: str = Field(description="ì‹¤ì œ ì½”ë“œ")
    explanation: str = Field(description="ì½”ë“œ ì„¤ëª…")

class CodeResponse(BaseModel):
    examples: List[CodeExample] = Field(description="ì½”ë“œ ì˜ˆì‹œë“¤")
    summary: str = Field(description="ì „ì²´ ìš”ì•½")

# JSON íŒŒì„œ ì„¤ì •
parser = JsonOutputParser(pydantic_object=CodeResponse)

# ì²´ì¸ì— íŒŒì„œ ì¶”ê°€
chain_with_parser = (
    {"question": RunnablePassthrough()} 
    | prompt 
    | llm 
    | parser
)

# êµ¬ì¡°í™”ëœ ì‘ë‹µ ë°›ê¸°
structured_result = chain_with_parser.invoke("Python ê°€ìƒí™˜ê²½ ì„¤ì • ì½”ë“œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”")
```

#### 6. ì‹¤ë¬´ í™œìš© íŒ¨í„´

**A. ë‹¨ê³„ë³„ ì‘ì—… ë¶„í•´**
```python
# 1ë‹¨ê³„: ìš”êµ¬ì‚¬í•­ ë¶„ì„
analysis_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ë‹¨ê³„ë“¤ì„ ë‚˜ì—´í•´ì£¼ì„¸ìš”: {requirement}"
)

# 2ë‹¨ê³„: ê° ë‹¨ê³„ë³„ ìƒì„¸ ì„¤ëª…
detail_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ ë‹¨ê³„ì— ëŒ€í•´ ìì„¸í•œ ì„¤ëª…ê³¼ ì½”ë“œ ì˜ˆì‹œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”: {step}"
)

# 3ë‹¨ê³„: ìµœì¢… ê²€ì¦
validation_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ í•´ê²°ë°©ì•ˆì´ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦í•˜ê³  ê°œì„ ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”: {solution}"
)
```

**B. ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„**
```python
from langchain_core.runnables import RunnableRetry

# ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì²´ì¸
robust_chain = (
    {"question": RunnablePassthrough()} 
    | prompt 
    | llm 
    | RunnableRetry(
        stop_after_attempt=3,
        wait=1
    )
    | parser
)
```

#### 7. ì„±ëŠ¥ ìµœì í™” íŒ

1. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬
2. **ìºì‹±**: ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ì¬ì‚¬ìš©
3. **ìŠ¤íŠ¸ë¦¬ë°**: ê¸´ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ê¸°
4. **í”„ë¡¬í”„íŠ¸ ì••ì¶•**: ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°í•˜ì—¬ í† í° ì ˆì•½

#### 8. ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

```python
from langchain_core.callbacks import ConsoleCallbackHandler

# ì½œë°±ì„ í†µí•œ ì‹¤í–‰ ê³¼ì • ëª¨ë‹ˆí„°ë§
with ConsoleCallbackHandler() as handler:
    result = chain.invoke("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸", config={"callbacks": [handler]})
```

#### 9. í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬

```python
import os
from dotenv import load_dotenv

load_dotenv()

# í™˜ê²½ë³„ ëª¨ë¸ ì„ íƒ
if os.getenv("ENVIRONMENT") == "production":
    llm = ChatOpenAI(model="gpt-4", temperature=0.1)
else:
    llm = ChatOllama(model="gemma3:1b", temperature=0.7)
```

#### 10. ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì ì ˆí•œ LLM ëª¨ë¸ ì„ íƒ (ë¡œì»¬ vs í´ë¼ìš°ë“œ)
- [ ] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ê³„ ë° ê²€ì¦
- [ ] ì¶œë ¥ íŒŒì„œ ì„¤ì • ë° í…ŒìŠ¤íŠ¸
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
- [ ] í™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬
- [ ] ë³´ì•ˆ ë° API í‚¤ ê´€ë¦¬

### ë‹µë³€ì˜ í˜•ì‹ì„ ì»¨íŠ¸ë¡¤í•˜ëŠ” ë°©ë²•

### LCELì„ í™œìš©í•œ ë­"ì²´ì¸" ìƒì„±í•˜ëŠ” ë°©ë²•

### ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ë‚´ìš© ì´ì •ë¦¬ (feat. í”„ë¡¬í”„íŠ¸ ê¿€íŒ ì‚´ì§)

### ë­ì²´ì¸ì„ í™œìš©í•´ì„œ í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤
