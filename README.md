# LangChain ê¸°ë³¸ê¸° í•™ìŠµ í”„ë¡œì íŠ¸

## ğŸ“š í”„ë¡œì íŠ¸ ê°œìš”
- **ê°•ì˜ëª…**: í•œì‹œê°„ìœ¼ë¡œ ëë‚´ëŠ” LangChain ê¸°ë³¸ê¸°
- **ëª©í‘œ**: LangChainì˜ í•µì‹¬ ê°œë…ì„ ì‹¤ìŠµì„ í†µí•´ í•™ìŠµ
- **Python ë²„ì „**: 3.12+
- **íŒ¨í‚¤ì§€ ê´€ë¦¬**: uv (ê¶Œì¥) ë˜ëŠ” pip

## ğŸš€ 1. í”„ë¡œì íŠ¸ ì„¤ì •

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- Python 3.12 ì´ìƒ
- uv (ê¶Œì¥) ë˜ëŠ” pip
- Ollama (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ)

### ë¹ ë¥¸ ì‹œì‘
```bash
# 1. í”„ë¡œì íŠ¸ ì„¤ì • (ìµœì´ˆ 1íšŒ)
./setup.sh

# 2. ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì‹¤í–‰
./run.sh
```

### ìˆ˜ë™ ì„¤ì •
```bash
# uv ì‚¬ìš© (ê¶Œì¥)
uv venv --python 3.12 .venv
source .venv/bin/activate
uv pip install -r requirements.txt

# ë˜ëŠ” pip ì‚¬ìš©
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Ollama ì„¤ì • (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ)
```bash
# Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
brew install ollama
ollama pull gemma3:1b
ollama serve
```

## ğŸ“– 2. í•™ìŠµ ë…¸íŠ¸ë¶ ê°€ì´ë“œ

### 01_chat.ipynb - LLM ê¸°ë³¸ ì‚¬ìš©ë²•
**í•µì‹¬ ë‚´ìš©**: ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ ì‚¬ìš©í•œ ê¸°ë³¸ì ì¸ ì±„íŒ… êµ¬í˜„

**í•™ìŠµ í¬ì¸íŠ¸**:
- **Ollama (ë¡œì»¬)**: `ChatOllama`ë¥¼ ì‚¬ìš©í•œ ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ
- **OpenAI (í´ë¼ìš°ë“œ)**: `ChatOpenAI`ë¥¼ ì‚¬ìš©í•œ í´ë¼ìš°ë“œ ëª¨ë¸ í˜¸ì¶œ
- **Azure OpenAI**: `AzureOpenAI`ë¥¼ ì‚¬ìš©í•œ ì—”í„°í”„ë¼ì´ì¦ˆ ëª¨ë¸ í˜¸ì¶œ

**ì£¼ìš” ì½”ë“œ**:
```python
# ë¡œì»¬ ëª¨ë¸
from langchain_ollama import ChatOllama
llm = ChatOllama(model="gemma3:1b")
response = llm.invoke("ë„ˆëŠ” ëˆ„êµ¬ëƒ?")

# í´ë¼ìš°ë“œ ëª¨ë¸
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o-mini")
response = llm.invoke("ë„ˆëŠ” ëˆ„êµ¬ëƒ?")
```

### 02_prompt.ipynb - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™œìš©
**í•µì‹¬ ë‚´ìš©**: LangChainì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‹œìŠ¤í…œ í•™ìŠµ

**í•™ìŠµ í¬ì¸íŠ¸**:
- **PromptTemplate**: ê¸°ë³¸ í…œí”Œë¦¿ ìƒì„± ë° ì‚¬ìš©
- **ChatPromptTemplate**: ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- **Message Types**: HumanMessage, AIMessage, SystemMessage í™œìš©

**ì£¼ìš” ì½”ë“œ**:
```python
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate

# ê¸°ë³¸ í…œí”Œë¦¿
prompt_template = PromptTemplate(
    input_variables=["country"], 
    template="What is the capital of {country}?"
)

# ì±„íŒ… í…œí”Œë¦¿
chat_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "What is the capital of {country}?")
])
```

### 03_output_parser.ipynb - ì¶œë ¥ íŒŒì‹± ë° êµ¬ì¡°í™”
**í•µì‹¬ ë‚´ìš©**: LLM ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•

**í•™ìŠµ í¬ì¸íŠ¸**:
- **StrOutputParser**: ë¬¸ìì—´ ì¶œë ¥ íŒŒì‹±
- **Pydantic ëª¨ë¸**: êµ¬ì¡°í™”ëœ ì¶œë ¥ ì •ì˜
- **with_structured_output**: ìë™ êµ¬ì¡°í™” ì¶œë ¥

**ì£¼ìš” ì½”ë“œ**:
```python
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field

# ê¸°ë³¸ íŒŒì„œ
output_parser = StrOutputParser()
result = output_parser.invoke(llm_response)

# êµ¬ì¡°í™”ëœ ì¶œë ¥
class CountryDetail(BaseModel):
    capital: str = Field(description="The capital of the country")
    population: int = Field(description="The population of the country")

structured_llm = llm.with_structured_output(CountryDetail)
```

### 04_runnable.ipynb - ì²´ì¸(Chain) êµ¬ì„±
**í•µì‹¬ ë‚´ìš©**: LangChainì˜ í•µì‹¬ì¸ Runnableê³¼ ì²´ì¸ êµ¬ì„±

**í•™ìŠµ í¬ì¸íŠ¸**:
- **Runnable íŒŒì´í”„ë¼ì¸**: `|` ì—°ì‚°ìë¥¼ ì‚¬ìš©í•œ ì²´ì¸ êµ¬ì„±
- **RunnablePassthrough**: ë°ì´í„° ì „ë‹¬ ë° ë³€í™˜
- **ë³µí•© ì²´ì¸**: ì—¬ëŸ¬ ì²´ì¸ì„ ì—°ê²°í•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°

**ì£¼ìš” ì½”ë“œ**:
```python
from langchain_core.runnables import RunnablePassthrough

# ê¸°ë³¸ ì²´ì¸
chain = prompt_template | llm | output_parser

# ë³µí•© ì²´ì¸
final_chain = (
    {"information": RunnablePassthrough()}
    | {"country": country_chain}
    | capital_chain
)
```

## ğŸ¯ 3. ìµœì¢… í”„ë¡œì íŠ¸: 05_review.ipynb

### í”„ë¡œì íŠ¸ ê°œìš”
**"êµ­ê°€ë³„ ì¸ê¸° ìŒì‹ê³¼ ë ˆì‹œí”¼ ì¶”ì²œ ì‹œìŠ¤í…œ"**ì„ LangChainìœ¼ë¡œ êµ¬í˜„í•œ ì¢…í•© í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

### ğŸ½ï¸ êµ¬í˜„ëœ ê¸°ëŠ¥

#### 1. ìŒì‹ ì¶”ì²œ ì‹œìŠ¤í…œ
```python
food_prompt = PromptTemplate(
    template="""What is one of the most popular food in {country}?
    Please return the name of the food only""",
    input_variables=["country"],
)

food_chain = food_prompt | llm | StrOutputParser()
```

#### 2. ë ˆì‹œí”¼ ìƒì„± ì‹œìŠ¤í…œ
```python
recipe_prompt = ChatPromptTemplate.from_messages([
    ("system", """Provide the recipe of the food that the user wants.
Please return the recipe only as a numbered list"""),
    ("human", "Can you give me the recipe for making {food}?"),
])

recipe_chain = recipe_prompt | llm | StrOutputParser()
```

#### 3. í†µí•© ì²´ì¸ ì‹œìŠ¤í…œ
```python
# ë‘ ì²´ì¸ì„ ì—°ê²°í•˜ì—¬ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±
final_chain = {"food": food_chain} | recipe_chain

# ì‚¬ìš© ì˜ˆì‹œ
result = final_chain.invoke({"country": "South Korea"})
# ì¶œë ¥: í•œêµ­ì˜ ì¸ê¸° ìŒì‹(ë¹„ë¹”ë°¥)ê³¼ ê·¸ ë ˆì‹œí”¼ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±
```

### ğŸš€ í•µì‹¬ í•™ìŠµ ì„±ê³¼

#### **LangChain í•µì‹¬ ê°œë… ì™„ì „ ì´í•´**
1. **LLM í†µí•©**: Ollama, OpenAI ë“± ë‹¤ì–‘í•œ ëª¨ë¸ í™œìš©
2. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ê³„
3. **ì¶œë ¥ êµ¬ì¡°í™”**: Pydanticì„ í™œìš©í•œ ë°ì´í„° ê²€ì¦
4. **ì²´ì¸ êµ¬ì„±**: Runnableì„ í†µí•œ ëª¨ë“ˆí™”ëœ ì›Œí¬í”Œë¡œìš°

#### **ì‹¤ë¬´ ì ìš© ê°€ëŠ¥í•œ íŒ¨í„´**
- **ëª¨ë“ˆí™”**: ê° ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ ì²´ì¸ìœ¼ë¡œ ë¶„ë¦¬
- **ì¬ì‚¬ìš©ì„±**: í…œí”Œë¦¿ê³¼ ì²´ì¸ì˜ ì¡°í•©ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì²´ì¸ ì¶”ê°€ë¡œ ê¸°ëŠ¥ í™•ì¥ ìš©ì´
- **ì—ëŸ¬ ì²˜ë¦¬**: ê° ë‹¨ê³„ë³„ ê²€ì¦ ë° ì˜¤ë¥˜ ì²˜ë¦¬

### ğŸ“ í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### **ê¸°ë³¸ ê°œë…**
- [ ] LLM ëª¨ë¸ ì´ˆê¸°í™” ë° í˜¸ì¶œ
- [ ] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ë° í™œìš©
- [ ] ì¶œë ¥ íŒŒì„œë¥¼ í†µí•œ ë°ì´í„° ë³€í™˜
- [ ] ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰

#### **ê³ ê¸‰ í™œìš©**
- [ ] êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±
- [ ] ë³µí•© ì²´ì¸ êµ¬ì„±
- [ ] ë°ì´í„° ì „ë‹¬ ë° ë³€í™˜
- [ ] ì‹¤ë¬´ í”„ë¡œì íŠ¸ êµ¬í˜„

#### **ì‹¤ë¬´ ì ìš©**
- [ ] ëª¨ë“ˆí™”ëœ ì½”ë“œ ì„¤ê³„
- [ ] ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ ê°œë°œ
- [ ] í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ êµ¬ì¶•
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ê²€ì¦ ë¡œì§

## ğŸ› ï¸ ê°œë°œ í™˜ê²½

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
langchain-basics-study/
â”œâ”€â”€ 01_chat.ipynb          # LLM ê¸°ë³¸ ì‚¬ìš©ë²•
â”œâ”€â”€ 02_prompt.ipynb        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”œâ”€â”€ 03_output_parser.ipynb # ì¶œë ¥ íŒŒì‹±
â”œâ”€â”€ 04_runnable.ipynb      # ì²´ì¸ êµ¬ì„±
â”œâ”€â”€ 05_review.ipynb        # ìµœì¢… í”„ë¡œì íŠ¸
â”œâ”€â”€ requirements.txt       # ì˜ì¡´ì„± ê´€ë¦¬
â”œâ”€â”€ setup.sh               # í”„ë¡œì íŠ¸ ì„¤ì •
â”œâ”€â”€ run.sh                 # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ clean.sh               # ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
```

### í¸ì˜ ìŠ¤í¬ë¦½íŠ¸
```bash
# í”„ë¡œì íŠ¸ ì„¤ì •
./setup.sh

# ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì‹¤í–‰
./run.sh

# í”„ë¡œì íŠ¸ ì •ë¦¬
./clean.sh
```

## ğŸ”§ í™˜ê²½ ì„¤ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ë³¸ í™˜ê²½
- [ ] Python 3.12 ì´ìƒ ì„¤ì¹˜
- [ ] uv ì„¤ì¹˜ (`curl -LsSf https://astral.sh/uv/install.sh | sh`)
- [ ] ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
- [ ] ì˜ì¡´ì„± ì„¤ì¹˜ (`uv pip install -r requirements.txt`)
- [ ] ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì‹¤í–‰

### ì„œë¹„ìŠ¤ ì„¤ì •
- [ ] í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼)
- [ ] Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰ (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ)
- [ ] API í‚¤ ì„¤ì • (í´ë¼ìš°ë“œ ëª¨ë¸ ì‚¬ìš© ì‹œ)

## ğŸ‰ ê²°ë¡ 

ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ LangChainì˜ í•µì‹¬ ê°œë…ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ **ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜**ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

**05_review.ipynb**ëŠ” ë‹¨ìˆœí•œ í•™ìŠµì„ ë„˜ì–´ì„œ **ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” íŒ¨í„´**ì„ ë³´ì—¬ì£¼ë©°, LangChainì˜ ì§„ì •í•œ ê°€ì¹˜ì¸ **ëª¨ë“ˆí™”, ì¬ì‚¬ìš©ì„±, í™•ì¥ì„±**ì„ ì²´í—˜í•  ìˆ˜ ìˆëŠ” ì™„ì„±ë„ ë†’ì€ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

---

**ğŸš€ ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•˜ì„¸ìš”!**
```bash
./setup.sh && ./run.sh
```